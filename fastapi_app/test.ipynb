{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SAMConfig(sam2_checkpoint='../sam2/checkpoints/sam2.1_hiera_large.pt', model_cfg='../sam2/configs/sam2.1/sam2.1_hiera_l.yaml')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from constant import CONFIG\n",
    "\n",
    "CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "sam2_checkpoint = CONFIG.sam2_checkpoint\n",
    "model_cfg = CONFIG.model_cfg\n",
    "\n",
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames extracted and saved to temp\\test\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "from utils.split_video import split_video\n",
    "\n",
    "\n",
    "# Path to the video file\n",
    "video_path = Path(r\"C:\\Users\\AORUS\\Desktop\\program\\gif_generator\\assests\\videoplayback.mp4\")\n",
    "split_video(video_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 51/51 [00:01<00:00, 42.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from constant import SPLIT_OUTPUT\n",
    "inference_state = predictor.init_state(video_path=SPLIT_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.reset_state(inference_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AORUS\\Desktop\\program\\gif_generator\\sam2\\sam2\\modeling\\sam\\transformer.py:270: UserWarning: Memory efficient kernel not used because: (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:773.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "C:\\Users\\AORUS\\Desktop\\program\\gif_generator\\sam2\\sam2\\modeling\\sam\\transformer.py:270: UserWarning: Memory Efficient attention has been runtime disabled. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen/native/transformers/sdp_utils_cpp.h:558.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "C:\\Users\\AORUS\\Desktop\\program\\gif_generator\\sam2\\sam2\\modeling\\sam\\transformer.py:270: UserWarning: Flash attention kernel not used because: (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:775.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "C:\\Users\\AORUS\\Desktop\\program\\gif_generator\\sam2\\sam2\\modeling\\sam\\transformer.py:270: UserWarning: Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:599.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "C:\\Users\\AORUS\\Desktop\\program\\gif_generator\\sam2\\sam2\\modeling\\sam\\transformer.py:270: UserWarning: CuDNN attention kernel not used because: (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:777.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "C:\\Users\\AORUS\\Desktop\\program\\gif_generator\\sam2\\sam2\\modeling\\sam\\transformer.py:270: UserWarning: Expected query, key and value to all be of dtype: {Half, BFloat16}. Got Query dtype: float, Key dtype: float, and Value dtype: float instead. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen/native/transformers/sdp_utils_cpp.h:110.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "c:\\Users\\AORUS\\Desktop\\program\\gif_generator\\sam\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: UserWarning: Flash Attention kernel failed due to: No available kernel. Aborting execution.\n",
      "Falling back to all available kernels for scaled_dot_product_attention (which may have a slower speed).\n",
      "  return forward_call(*args, **kwargs)\n",
      "C:\\Users\\AORUS\\Desktop\\program\\gif_generator\\sam2\\sam2\\sam2_video_predictor.py:961: UserWarning: cannot import name '_C' from 'sam2' (C:\\Users\\AORUS\\Desktop\\program\\gif_generator\\sam2\\sam2\\__init__.py)\n",
      "\n",
      "Skipping the post-processing step due to the error above. You can still use SAM 2 and it's OK to ignore the error above, although some post-processing functionality may be limited (which doesn't affect the results in most cases; see https://github.com/facebookresearch/sam2/blob/main/INSTALL.md).\n",
      "  pred_masks_gpu = fill_holes_in_mask_scores(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " [1],\n",
       " tensor([[[[ -9.3192,  -9.9280, -10.5117,  ..., -11.5058, -11.7148, -12.9639],\n",
       "           [ -9.5097, -10.0291, -10.5796,  ..., -11.6492, -11.7984, -12.7516],\n",
       "           [-10.2717, -10.4337, -10.8509,  ..., -12.2229, -12.1329, -11.9023],\n",
       "           ...,\n",
       "           [-12.4386, -12.7320, -13.7569,  ..., -15.0166, -14.7487, -13.8335],\n",
       "           [-10.8838, -12.3968, -13.9308,  ..., -14.8937, -14.4332, -13.6659],\n",
       "           [-10.4952, -12.3131, -13.9743,  ..., -14.8630, -14.3544, -13.6239]]]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's add a positive click at (x, y) = (210, 350) to get started\n",
    "points = np.array([[210, 350]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([1], np.int32)\n",
    "predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points,\n",
    "    labels=labels,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "propagate in video: 100%|██████████| 51/51 [00:26<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.segement import video_segement\n",
    "\n",
    "video_segements = video_segement(predictor=predictor, inference_state=inference_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(video_segements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image loaded successfully. Shape: (640, 360, 3)\n",
      "Image loaded successfully. Shape: (640, 360, 3)\n",
      "Image loaded successfully. Shape: (640, 360, 3)\n",
      "Image loaded successfully. Shape: (640, 360, 3)\n",
      "Image loaded successfully. Shape: (640, 360, 3)\n",
      "Image loaded successfully. Shape: (640, 360, 3)\n",
      "Image loaded successfully. Shape: (640, 360, 3)\n",
      "Image loaded successfully. Shape: (640, 360, 3)\n",
      "Image loaded successfully. Shape: (640, 360, 3)\n",
      "Image loaded successfully. Shape: (640, 360, 3)\n",
      "Image loaded successfully. Shape: (640, 360, 3)\n"
     ]
    }
   ],
   "source": [
    "from utils.show_mask import show_mask\n",
    "\n",
    "temp = show_mask(frame_stride=5,video_segements=video_segements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 360, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshow_img\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_image\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m temp:\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mshow_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AORUS\\Desktop\\program\\gif_generator\\fastapi_app\\utils\\show_img.py:8\u001b[0m, in \u001b[0;36mshow_image\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m      6\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m      7\u001b[0m img \u001b[38;5;241m=\u001b[39m  Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AORUS\\Desktop\\program\\gif_generator\\sam\\Lib\\site-packages\\PIL\\Image.py:2494\u001b[0m, in \u001b[0;36mImage.show\u001b[1;34m(self, title)\u001b[0m\n\u001b[0;32m   2474\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2475\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2476\u001b[0m \u001b[38;5;124;03m    Displays this image. This method is mainly intended for debugging purposes.\u001b[39;00m\n\u001b[0;32m   2477\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2491\u001b[0m \u001b[38;5;124;03m    :param title: Optional title to use for the image window, where possible.\u001b[39;00m\n\u001b[0;32m   2492\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2494\u001b[0m     \u001b[43m_show\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtitle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AORUS\\Desktop\\program\\gif_generator\\sam\\Lib\\site-packages\\PIL\\Image.py:3539\u001b[0m, in \u001b[0;36m_show\u001b[1;34m(image, **options)\u001b[0m\n\u001b[0;32m   3536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_show\u001b[39m(image, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions):\n\u001b[0;32m   3537\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageShow\n\u001b[1;32m-> 3539\u001b[0m     \u001b[43mImageShow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AORUS\\Desktop\\program\\gif_generator\\sam\\Lib\\site-packages\\PIL\\ImageShow.py:62\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(image, title, **options)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03mDisplay a given image.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m:returns: ``True`` if a suitable viewer was found, ``False`` otherwise.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m viewer \u001b[38;5;129;01min\u001b[39;00m _viewers:\n\u001b[1;32m---> 62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\AORUS\\Desktop\\program\\gif_generator\\sam\\Lib\\site-packages\\PIL\\ImageShow.py:86\u001b[0m, in \u001b[0;36mViewer.show\u001b[1;34m(self, image, **options)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m!=\u001b[39m base:\n\u001b[0;32m     84\u001b[0m         image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mconvert(base)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AORUS\\Desktop\\program\\gif_generator\\sam\\Lib\\site-packages\\PIL\\ImageShow.py:113\u001b[0m, in \u001b[0;36mViewer.show_image\u001b[1;34m(self, image, **options)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions):\n\u001b[0;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display the given image.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AORUS\\Desktop\\program\\gif_generator\\sam\\Lib\\site-packages\\PIL\\ImageShow.py:119\u001b[0m, in \u001b[0;36mViewer.show_file\u001b[1;34m(self, path, **options)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow_file\u001b[39m(\u001b[38;5;28mself\u001b[39m, path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions):\n\u001b[0;32m    116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    Display given file.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# nosec\u001b[39;00m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils.show_img import show_image\n",
    "\n",
    "\n",
    "for i in temp:\n",
    "    show_image(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 0000_masked.png as PNG\n",
      "Saved 0001_masked.png as PNG\n",
      "Saved 0002_masked.png as PNG\n",
      "Saved 0003_masked.png as PNG\n",
      "Saved 0004_masked.png as PNG\n",
      "Saved 0005_masked.png as PNG\n",
      "Saved 0006_masked.png as PNG\n",
      "Saved 0007_masked.png as PNG\n",
      "Saved 0008_masked.png as PNG\n",
      "Saved 0009_masked.png as PNG\n",
      "Saved 0010_masked.png as PNG\n",
      "Saved 0011_masked.png as PNG\n",
      "Saved 0012_masked.png as PNG\n",
      "Saved 0013_masked.png as PNG\n",
      "Saved 0014_masked.png as PNG\n",
      "Saved 0015_masked.png as PNG\n",
      "Saved 0016_masked.png as PNG\n",
      "Saved 0017_masked.png as PNG\n",
      "Saved 0018_masked.png as PNG\n",
      "Saved 0019_masked.png as PNG\n",
      "Saved 0020_masked.png as PNG\n",
      "Saved 0021_masked.png as PNG\n",
      "Saved 0022_masked.png as PNG\n",
      "Saved 0023_masked.png as PNG\n",
      "Saved 0024_masked.png as PNG\n",
      "Saved 0025_masked.png as PNG\n",
      "Saved 0026_masked.png as PNG\n",
      "Saved 0027_masked.png as PNG\n",
      "Saved 0028_masked.png as PNG\n",
      "Saved 0029_masked.png as PNG\n",
      "Saved 0030_masked.png as PNG\n",
      "Saved 0031_masked.png as PNG\n",
      "Saved 0032_masked.png as PNG\n",
      "Saved 0033_masked.png as PNG\n",
      "Saved 0034_masked.png as PNG\n",
      "Saved 0035_masked.png as PNG\n",
      "Saved 0036_masked.png as PNG\n",
      "Saved 0037_masked.png as PNG\n",
      "Saved 0038_masked.png as PNG\n",
      "Saved 0039_masked.png as PNG\n",
      "Saved 0040_masked.png as PNG\n",
      "Saved 0041_masked.png as PNG\n",
      "Saved 0042_masked.png as PNG\n",
      "Saved 0043_masked.png as PNG\n",
      "Saved 0044_masked.png as PNG\n",
      "Saved 0045_masked.png as PNG\n",
      "Saved 0046_masked.png as PNG\n",
      "Saved 0047_masked.png as PNG\n",
      "Saved 0048_masked.png as PNG\n",
      "Saved 0049_masked.png as PNG\n",
      "Saved 0050_masked.png as PNG\n"
     ]
    }
   ],
   "source": [
    "from utils.crop_img import crop_mask\n",
    "\n",
    "crop_mask(video_segments=video_segements, frame_len=51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF saved as 'temp\\gif\\output.gif'\n"
     ]
    }
   ],
   "source": [
    "from utils.to_gif import to_gif\n",
    "\n",
    "to_gif(51,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
