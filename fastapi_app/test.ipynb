{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SAMConfig(sam2_checkpoint='../sam2/checkpoints/sam2.1_hiera_large.pt', model_cfg='../sam2/configs/sam2.1/sam2.1_hiera_l.yaml')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from constant import CONFIG\n",
    "\n",
    "CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "sam2_checkpoint = CONFIG.sam2_checkpoint\n",
    "model_cfg = CONFIG.model_cfg\n",
    "\n",
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames extracted and saved to temp\\test\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "from utils.split_video import split_video\n",
    "\n",
    "\n",
    "# Path to the video file\n",
    "video_path = Path(r\"C:\\Users\\AORUS\\Desktop\\program\\gif_generator\\assests\\videoplayback.mp4\")\n",
    "split_video(video_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 51/51 [00:01<00:00, 37.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from constant import SPLIT_OUTPUT\n",
    "inference_state = predictor.init_state(video_path=SPLIT_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.reset_state(inference_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AORUS\\Desktop\\program\\gif_generator\\sam2\\sam2\\modeling\\sam\\transformer.py:270: UserWarning: Memory efficient kernel not used because: (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:773.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "C:\\Users\\AORUS\\Desktop\\program\\gif_generator\\sam2\\sam2\\modeling\\sam\\transformer.py:270: UserWarning: Memory Efficient attention has been runtime disabled. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen/native/transformers/sdp_utils_cpp.h:558.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "C:\\Users\\AORUS\\Desktop\\program\\gif_generator\\sam2\\sam2\\modeling\\sam\\transformer.py:270: UserWarning: Flash attention kernel not used because: (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:775.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "C:\\Users\\AORUS\\Desktop\\program\\gif_generator\\sam2\\sam2\\modeling\\sam\\transformer.py:270: UserWarning: Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:599.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "C:\\Users\\AORUS\\Desktop\\program\\gif_generator\\sam2\\sam2\\modeling\\sam\\transformer.py:270: UserWarning: CuDNN attention kernel not used because: (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:777.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "C:\\Users\\AORUS\\Desktop\\program\\gif_generator\\sam2\\sam2\\modeling\\sam\\transformer.py:270: UserWarning: Expected query, key and value to all be of dtype: {Half, BFloat16}. Got Query dtype: float, Key dtype: float, and Value dtype: float instead. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen/native/transformers/sdp_utils_cpp.h:110.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
      "c:\\Users\\AORUS\\Desktop\\program\\gif_generator\\sam\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: UserWarning: Flash Attention kernel failed due to: No available kernel. Aborting execution.\n",
      "Falling back to all available kernels for scaled_dot_product_attention (which may have a slower speed).\n",
      "  return forward_call(*args, **kwargs)\n",
      "C:\\Users\\AORUS\\Desktop\\program\\gif_generator\\sam2\\sam2\\sam2_video_predictor.py:961: UserWarning: cannot import name '_C' from 'sam2' (C:\\Users\\AORUS\\Desktop\\program\\gif_generator\\sam2\\sam2\\__init__.py)\n",
      "\n",
      "Skipping the post-processing step due to the error above. You can still use SAM 2 and it's OK to ignore the error above, although some post-processing functionality may be limited (which doesn't affect the results in most cases; see https://github.com/facebookresearch/sam2/blob/main/INSTALL.md).\n",
      "  pred_masks_gpu = fill_holes_in_mask_scores(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " [1],\n",
       " tensor([[[[ -9.3192,  -9.9280, -10.5117,  ..., -11.5058, -11.7148, -12.9639],\n",
       "           [ -9.5097, -10.0291, -10.5796,  ..., -11.6492, -11.7984, -12.7516],\n",
       "           [-10.2717, -10.4337, -10.8509,  ..., -12.2229, -12.1329, -11.9023],\n",
       "           ...,\n",
       "           [-12.4386, -12.7320, -13.7569,  ..., -15.0166, -14.7487, -13.8335],\n",
       "           [-10.8838, -12.3968, -13.9308,  ..., -14.8937, -14.4332, -13.6659],\n",
       "           [-10.4952, -12.3131, -13.9743,  ..., -14.8630, -14.3544, -13.6239]]]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's add a positive click at (x, y) = (210, 350) to get started\n",
    "points = np.array([[210, 350]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([1], np.int32)\n",
    "predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points,\n",
    "    labels=labels,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "propagate in video: 100%|██████████| 51/51 [00:27<00:00,  1.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.segement import video_segement\n",
    "\n",
    "video_segements = video_segement(predictor=predictor, inference_state=inference_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 0000_masked.png as PNG\n",
      "Saved 0001_masked.png as PNG\n",
      "Saved 0002_masked.png as PNG\n",
      "Saved 0003_masked.png as PNG\n",
      "Saved 0004_masked.png as PNG\n",
      "Saved 0005_masked.png as PNG\n",
      "Saved 0006_masked.png as PNG\n",
      "Saved 0007_masked.png as PNG\n",
      "Saved 0008_masked.png as PNG\n",
      "Saved 0009_masked.png as PNG\n",
      "Saved 0010_masked.png as PNG\n",
      "Saved 0011_masked.png as PNG\n",
      "Saved 0012_masked.png as PNG\n",
      "Saved 0013_masked.png as PNG\n",
      "Saved 0014_masked.png as PNG\n",
      "Saved 0015_masked.png as PNG\n",
      "Saved 0016_masked.png as PNG\n",
      "Saved 0017_masked.png as PNG\n",
      "Saved 0018_masked.png as PNG\n",
      "Saved 0019_masked.png as PNG\n",
      "Saved 0020_masked.png as PNG\n",
      "Saved 0021_masked.png as PNG\n",
      "Saved 0022_masked.png as PNG\n",
      "Saved 0023_masked.png as PNG\n",
      "Saved 0024_masked.png as PNG\n",
      "Saved 0025_masked.png as PNG\n",
      "Saved 0026_masked.png as PNG\n",
      "Saved 0027_masked.png as PNG\n",
      "Saved 0028_masked.png as PNG\n",
      "Saved 0029_masked.png as PNG\n",
      "Saved 0030_masked.png as PNG\n",
      "Saved 0031_masked.png as PNG\n",
      "Saved 0032_masked.png as PNG\n",
      "Saved 0033_masked.png as PNG\n",
      "Saved 0034_masked.png as PNG\n",
      "Saved 0035_masked.png as PNG\n",
      "Saved 0036_masked.png as PNG\n",
      "Saved 0037_masked.png as PNG\n",
      "Saved 0038_masked.png as PNG\n",
      "Saved 0039_masked.png as PNG\n",
      "Saved 0040_masked.png as PNG\n",
      "Saved 0041_masked.png as PNG\n",
      "Saved 0042_masked.png as PNG\n",
      "Saved 0043_masked.png as PNG\n",
      "Saved 0044_masked.png as PNG\n",
      "Saved 0045_masked.png as PNG\n",
      "Saved 0046_masked.png as PNG\n",
      "Saved 0047_masked.png as PNG\n",
      "Saved 0048_masked.png as PNG\n",
      "Saved 0049_masked.png as PNG\n",
      "Saved 0050_masked.png as PNG\n"
     ]
    }
   ],
   "source": [
    "from utils.crop_img import crop_mask\n",
    "\n",
    "crop_mask(video_segments=video_segements, frame_len=51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF saved as 'temp\\gif\\output.gif'\n"
     ]
    }
   ],
   "source": [
    "from utils.to_gif import to_gif\n",
    "\n",
    "to_gif(51,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
